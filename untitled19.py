# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovgemDIFrYxrxLG_PEhh385aC2MdMwQO
"""

!uv pip install fastapi uvicorn[standard] python-multipart pillow torch transformers numpy

from google.colab import files
import os

print("üì§ Please upload your index.html file:")
uploaded = files.upload()

for filename in uploaded.keys():
    print(f'‚úÖ Uploaded: {filename}')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# 
# from fastapi import FastAPI, File, UploadFile, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import JSONResponse, HTMLResponse
# from fastapi.staticfiles import StaticFiles
# from PIL import Image
# import torch
# from transformers import OwlViTProcessor, OwlViTForObjectDetection
# import io
# from typing import Dict
# import logging
# from contextlib import asynccontextmanager
# 
# # Configure logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
# 
# # Global variables to store the model
# processor = None
# model = None
# device = "cpu"
# 
# async def load_model_startup():
#     """Load the OwlViT model at startup"""
#     global processor, model, device
#     try:
#         logger.info("Loading OwlViT model and processor...")
#         processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
#         model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")
# 
#         # Use GPU if available
#         if torch.cuda.is_available():
#             device = "cuda"
#             logger.info("CUDA available - using GPU")
#         else:
#             device = "cpu"
#             logger.info("Using CPU")
# 
#         model.to(device)
#         model.eval()
#         logger.info(f"Model loaded successfully on {device}")
#     except Exception as e:
#         logger.error(f"Error loading model: {e}")
#         raise e
# 
# @asynccontextmanager
# async def lifespan(app: FastAPI):
#     """Lifespan context manager for startup/shutdown events"""
#     # Startup
#     logger.info("Application starting up...")
#     await load_model_startup()
#     yield
#     # Shutdown
#     logger.info("Application shutting down...")
# 
# # Initialize FastAPI with lifespan
# app = FastAPI(title="Shop Status Detection API", lifespan=lifespan)
# 
# # CORS middleware
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )
# 
# def classify_shop_status(image: Image.Image) -> Dict[str, any]:
#     """
#     Classifies whether a shop is open or closed based on the image.
# 
#     NO THRESHOLD - returns best detection regardless of confidence score.
#     This ensures we always get a result, even with low confidence.
# 
#     Args:
#         image: PIL Image object
# 
#     Returns:
#         Dictionary with status, message, confidence, and detected_label
#     """
#     if not processor or not model:
#         raise RuntimeError("Model not loaded")
# 
#     try:
#         # Define multiple text queries for better detection
#         # Note: OwlViT works better with descriptive phrases
#         text_queries = [
#             "an open shop entrance with visible interior",
#             "a closed shop with metal shutter pulled down",
#             "shop door open",
#             "shop shutter closed",
#             "store entrance open",
#             "rolling shutter down"
#         ]
#         texts = [text_queries]
# 
#         # Convert image to RGB
#         image = image.convert("RGB")
# 
#         # Perform inference
#         inputs = processor(text=texts, images=image, return_tensors="pt").to(device)
#         with torch.no_grad():
#             outputs = model(**inputs)
# 
#         # Post-process with NO THRESHOLD to get all detections
#         target_sizes = torch.Tensor([image.size[::-1]]).to(device)
#         results = processor.post_process_object_detection(
#             outputs=outputs,
#             threshold=0.0,  # No filtering - get everything!
#             target_sizes=target_sizes
#         )
# 
#         scores = results[0]["scores"]
#         labels = results[0]["labels"]
# 
#         logger.info(f"Total detections found: {len(scores)}")
# 
#         # Log top 5 detections for debugging
#         if len(scores) > 0:
#             top_n = min(5, len(scores))
#             logger.info("Top detections:")
#             for i in range(top_n):
#                 label_text = text_queries[labels[i].item()]
#                 score = scores[i].item()
#                 logger.info(f"  {i+1}. '{label_text}' - confidence: {score:.4f}")
# 
#         # Handle case with no detections
#         if len(scores) == 0:
#             logger.warning("No detections found - defaulting to 'open'")
#             return {
#                 "status": "open",
#                 "message": "Shop is Open (default - no strong indicators)",
#                 "confidence": 0.5,
#                 "detected_label": "no specific features detected"
#             }
# 
#         # Get the best detection (highest confidence)
#         best_idx = scores.argmax()
#         best_label_id = labels[best_idx].item()
#         best_score = scores[best_idx].item()
#         best_label = text_queries[best_label_id]
# 
#         logger.info(f"Best detection: '{best_label}' with confidence {best_score:.4f}")
# 
#         # Determine status based on detected label
#         # Keywords for closed status
#         closed_keywords = ["closed", "shutter", "down", "pulled down"]
#         # Keywords for open status
#         open_keywords = ["open", "entrance", "visible", "door"]
# 
#         label_lower = best_label.lower()
# 
#         # Check if it's closed
#         has_closed = any(keyword in label_lower for keyword in closed_keywords)
#         has_open = any(keyword in label_lower for keyword in open_keywords)
# 
#         # Determine final status
#         if has_closed and not has_open:
#             status = "closed"
#             message = "Shop is Closed"
#         elif has_open or not has_closed:
#             status = "open"
#             message = "Shop is Open"
#         else:
#             # Default to open if ambiguous
#             status = "open"
#             message = "Shop is Open"
# 
#         # Ensure confidence is at least 0.01 to avoid showing 0.00%
#         final_confidence = max(best_score, 0.01)
# 
#         return {
#             "status": status,
#             "message": message,
#             "confidence": round(float(final_confidence), 4),
#             "detected_label": best_label
#         }
# 
#     except Exception as e:
#         logger.error(f"Error during classification: {e}", exc_info=True)
#         # Return a safe default instead of raising error
#         return {
#             "status": "unknown",
#             "message": "Could not determine shop status",
#             "confidence": 0.0,
#             "detected_label": f"error: {str(e)}"
#         }
# 
# @app.post("/api/detect-shop-status")
# async def detect_shop_status(file: UploadFile = File(...)):
#     """
#     API endpoint to detect if a shop is open or closed from an uploaded image.
#     """
#     try:
#         # Validate file type
#         if not file.content_type.startswith("image/"):
#             raise HTTPException(status_code=400, detail="File must be an image")
# 
#         # Read and process the image
#         contents = await file.read()
#         image = Image.open(io.BytesIO(contents))
# 
#         logger.info(f"Processing image: size={image.size}, mode={image.mode}")
# 
#         # Classify the image
#         result = classify_shop_status(image)
# 
#         logger.info(f"Classification result: {result}")
#         return JSONResponse(content=result)
# 
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"Error processing request: {e}", exc_info=True)
#         raise HTTPException(status_code=500, detail=f"Error processing image: {str(e)}")
# 
# @app.get("/api/health")
# async def health_check():
#     """Health check endpoint"""
#     return {
#         "status": "healthy",
#         "model_loaded": model is not None and processor is not None,
#         "device": device
#     }
# 
# @app.get("/", response_class=HTMLResponse)
# async def serve_frontend():
#     """Serve the index.html file"""
#     try:
#         with open("index.html", "r", encoding="utf-8") as f:
#             return HTMLResponse(content=f.read())
#     except FileNotFoundError:
#         return HTMLResponse(
#             content="<h1>Shop Status Detector</h1><p>index.html not found. Please upload it.</p>",
#             status_code=404
#         )
# 
# # Alternative: Mount static files if you have a static folder
# # app.mount("/", StaticFiles(directory="static", html=True), name="static")

import nest_asyncio
import uvicorn
from threading import Thread
import time

# Apply nest_asyncio - this is crucial for Colab
nest_asyncio.apply()

# Define server function
def run_server():
    uvicorn.run("main:app", host="0.0.0.0", port=8000, log_level="info")

# Start server in background thread
print("üöÄ Starting server...")
thread = Thread(target=run_server, daemon=True)
thread.start()

# Wait for server to start
time.sleep(5)

print("\n" + "="*60)
print("‚úÖ Server is running on http://localhost:8000")
print("="*60)
print("\nüí° In Colab, you can access it using the Colab port forwarding feature")
print("üìù Look for the 'Open in new tab' icon next to the port 8000 output")
print("\n‚ö†Ô∏è  Keep this cell running to maintain the server!")

